import os
import random
import h5py
import numpy as np
import torch
from torchvision import datasets, transforms
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from typing import List, Dict, Tuple, Optional
import cv2

HDF5_DIR = './synthetic_dataset'
TRAIN_FILE = os.path.join(HDF5_DIR, 'train_data.h5')
TEST_FILE = os.path.join(HDF5_DIR, 'test_data.h5')
TOTAL_SAMPLES = 200000 # è®¾å®šçš„æ€»æ ·æœ¬æ•°
TEST_SPLIT_RATIO = 0.1 # æµ‹è¯•é›†æ¯”ä¾‹ 10%

PATCH_SIZE = 64
DIGIT_SIZE = 28
BACKGROUND_DIR = './data/pic'
MNIST_DATA_PATH = './data'


def load_and_prepare_mnist(root_path: str = MNIST_DATA_PATH):
    """åŠ è½½å¹¶å‡†å¤‡ MNIST æ•°æ®ï¼Œæå–å‰æ™¯å›¾å’Œæ©ç """
    transform = transforms.Compose([transforms.ToTensor()])
    mnist_dataset = datasets.MNIST(root=root_path, train=True, download=True, transform=transform)
    prepared_data = []
    for image_tensor, label in mnist_dataset:
        mnist_image = image_tensor.squeeze(0).numpy()
        digit_fg = mnist_image.copy()
        digit_mask = (mnist_image > 0).astype(np.float32)
        prepared_data.append({'fg': digit_fg, 'mask': digit_mask, 'label': label})
    return prepared_data


def load_background_images(bg_dir: str = BACKGROUND_DIR) -> List[np.ndarray]:
    """åŠ è½½èƒŒæ™¯å›¾ç‰‡å¹¶è½¬æ¢ä¸ºç°åº¦å›¾ NumPy æ•°ç»„"""
    bg_images = []
    for filename in os.listdir(bg_dir):
        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
            img = Image.open(os.path.join(bg_dir, filename)).convert('L')
            img_array = np.array(img, dtype=np.uint8)
            bg_images.append(img_array)
    return bg_images


def apply_augmentation(digit_fg: np.ndarray, digit_mask: np.ndarray, 
                       max_angle: float = 15) -> Tuple[np.ndarray, np.ndarray]:
    """
    å¯¹å‰æ™¯å›¾å’Œæ©ç åº”ç”¨éšæœºæ—‹è½¬ã€‚
    è¿”å›:
        augmented_fg: å¢å¼ºåçš„å‰æ™¯å›¾ (28x28)
        augmented_mask: å¢å¼ºåçš„æ©ç  (28x28)
    """
    H, W = DIGIT_SIZE, DIGIT_SIZE
    center = (W/2 - 0.5, H/2 - 0.5)
    angle = random.uniform(-max_angle, max_angle)
    M_rot = cv2.getRotationMatrix2D(center, angle, 1.0)
    augmented_fg = cv2.warpAffine(digit_fg, M_rot, (W, H), 
                                  flags=cv2.INTER_LINEAR, borderValue=0.0)
    augmented_mask = cv2.warpAffine(digit_mask, M_rot, (W, H), 
                                    flags=cv2.INTER_NEAREST, borderValue=0.0)
    augmented_mask = (augmented_mask > 0).astype(np.float32)   
    return augmented_fg, augmented_mask


def create_synthetic_sample(prepared_mnist_data: List[Dict], bg_images: List[np.ndarray], 
                            patch_size: int = PATCH_SIZE) -> Optional[Tuple[np.ndarray, np.ndarray, np.ndarray, int]]:
    """
    ä»èƒŒæ™¯å›¾å’Œ MNIST æ•°å­—ä¸­åˆ›å»ºå•ä¸ªåˆæˆæ ·æœ¬ï¼Œå¹¶åº”ç”¨æ•°æ®å¢å¼ºã€‚
    è¿”å›: synthetic_image, gt_mask_task1, gt_mask_task2, label
    """
    H, W = patch_size, patch_size
    
    if not bg_images: return None
    
    # 1. è£å‰ªèƒŒæ™¯å›¾
    bg_image = random.choice(bg_images)
    bg_H, bg_W = bg_image.shape
    if bg_H < H or bg_W < W: return None

    start_y = random.randint(0, bg_H - H)
    start_x = random.randint(0, bg_W - W)
    bg_patch = bg_image[start_y:start_y + H, start_x:start_x + W]
    bg_patch_float = bg_patch.astype(np.float32) / 255.0

    # 2. é€‰æ‹© MNIST æ ·æœ¬å¹¶åº”ç”¨æ•°æ®å¢å¼º
    mnist_sample = random.choice(prepared_mnist_data)
    digit_fg_orig = mnist_sample['fg']
    digit_mask_orig = mnist_sample['mask']
    label = mnist_sample['label']
    digit_fg, digit_mask = apply_augmentation(digit_fg_orig, digit_mask_orig)
    
    # 3. éšæœºå®šä½æ•°å­—åœ¨ Patch ä¸­çš„ä½ç½®
    max_x = W - DIGIT_SIZE
    max_y = H - DIGIT_SIZE
    pos_x = random.randint(0, max_x)
    pos_y = random.randint(0, max_y)
    
    # 4. å åŠ æ•°å­—åˆ°èƒŒæ™¯ Patch
    synthetic_image = bg_patch_float.copy()
    roi = synthetic_image[pos_y:pos_y + DIGIT_SIZE, pos_x:pos_x + DIGIT_SIZE]
    
    new_roi = roi * (1.0 - digit_mask) + digit_fg * digit_mask
    synthetic_image[pos_y:pos_y + DIGIT_SIZE, pos_x:pos_x + DIGIT_SIZE] = new_roi
    
    gt_mask_task1 = np.zeros((H, W), dtype=np.uint8) # ç”¨äºä»»åŠ¡1 (å‰æ™¯/èƒŒæ™¯)
    gt_mask_task2 = np.zeros((H, W), dtype=np.uint8) # ç”¨äºä»»åŠ¡2 (å¤šç±»åˆ«)
    
    # ä»»åŠ¡1ï¼šå‰æ™¯èƒŒæ™¯åˆ†å‰² (2åˆ†ç±»)
    gt_mask_task1[pos_y:pos_y + DIGIT_SIZE, pos_x:pos_x + DIGIT_SIZE] = (digit_mask > 0).astype(np.uint8)
    
    # ä»»åŠ¡2ï¼šæŒ‰ç…§æ•°å­—ç±»åˆ«åˆ†å‰² (11åˆ†ç±»: 0-èƒŒæ™¯, 1-æ•°å­—0, 2-æ•°å­—1, ..., 10-æ•°å­—9)
    mask_value = label + 1 # ç±»åˆ«æ˜ å°„ï¼š0->1, 1->2, ..., 9->10
    
    gt_mask_task2[pos_y:pos_y + DIGIT_SIZE, pos_x:pos_x + DIGIT_SIZE] = (digit_mask * mask_value).astype(np.uint8)

    return synthetic_image, gt_mask_task1, gt_mask_task2, label


def generate_and_save_dataset(total_samples: int, test_split_ratio: float):
    """ç”Ÿæˆæ•°æ®é›†å¹¶ä¿å­˜åˆ° HDF5 æ–‡ä»¶ã€‚"""
    os.makedirs(HDF5_DIR, exist_ok=True)
    
    print("â³ 1. æ­£åœ¨åŠ è½½ MNIST æ•°æ®...")
    prepared_mnist_data = load_and_prepare_mnist()
    print("â³ 2. æ­£åœ¨åŠ è½½èƒŒæ™¯å›¾ç‰‡...")
    bg_images = load_background_images()
    
    if not prepared_mnist_data or not bg_images:
        print("ğŸ›‘ ç¼ºå°‘å¿…è¦çš„æ•°æ®æº (MNIST æˆ–èƒŒæ™¯å›¾)ã€‚è¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return

    print(f"â³ 3. æ­£åœ¨ç”Ÿæˆ {total_samples} ä¸ªæ ·æœ¬ (å«æ•°æ®å¢å¼º)...")
    
    all_images = []
    all_masks_task1 = []
    all_masks_task2 = []
    all_labels = [] # <--- æ–°å¢ï¼šæ”¶é›†æ ‡ç­¾
    
    count = 0
    while count < total_samples:
        # ä¿®æ”¹ï¼šæ¥æ”¶ label
        sample = create_synthetic_sample(prepared_mnist_data, bg_images, PATCH_SIZE)
        if sample:
            img, mask1, mask2, label = sample # <--- è§£åŒ… label
            all_images.append(img)
            all_masks_task1.append(mask1)
            all_masks_task2.append(mask2)
            all_labels.append(label) # <--- å­˜å‚¨ label
            count += 1
            if count % 1000 == 0 or count == total_samples:
                print(f"   å·²ç”Ÿæˆ {count}/{total_samples} ä¸ªæ ·æœ¬...")
        else:
            continue

    print("â³ 4. æ­£åœ¨è½¬æ¢æ•°æ®å¹¶åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†...")
    
    all_images = np.array(all_images, dtype=np.float32)
    all_masks_task1 = np.array(all_masks_task1, dtype=np.uint8)
    all_masks_task2 = np.array(all_masks_task2, dtype=np.uint8)
    all_labels = np.array(all_labels, dtype=np.uint8) # <--- è½¬æ¢ labels
    
    num_test = int(total_samples * test_split_ratio)
    num_train = total_samples - num_test
    
    indices = np.arange(total_samples)
    np.random.shuffle(indices)
    
    train_indices = indices[:num_train]
    test_indices = indices[num_train:]

    train_images = all_images[train_indices]
    train_masks_task1 = all_masks_task1[train_indices]
    train_masks_task2 = all_masks_task2[train_indices]
    train_labels = all_labels[train_indices] # <--- åˆ’åˆ† labels
    
    test_images = all_images[test_indices]
    test_masks_task1 = all_masks_task1[test_indices]
    test_masks_task2 = all_masks_task2[test_indices]
    test_labels = all_labels[test_indices] # <--- åˆ’åˆ† labels
    
    print(f"   è®­ç»ƒé›†æ•°é‡: {num_train}, æµ‹è¯•é›†æ•°é‡: {num_test}")
    print("â³ 5. æ­£åœ¨ä¿å­˜ HDF5 æ–‡ä»¶...")

    def save_to_hdf5(file_path, images, masks_task1, masks_task2, labels):
        with h5py.File(file_path, 'w') as f:
            f.create_dataset('images', data=images, compression="gzip", compression_opts=9)
            f.create_dataset('masks_task1', data=masks_task1, compression="gzip", compression_opts=9)
            f.create_dataset('masks_task2', data=masks_task2, compression="gzip", compression_opts=9)
            f.create_dataset('labels', data=labels, compression="gzip", compression_opts=9) # <--- æ–°å¢ï¼šä¿å­˜ labels
        print(f"âœ… ä¿å­˜åˆ°: {file_path}")

    save_to_hdf5(TRAIN_FILE, train_images, train_masks_task1, train_masks_task2, train_labels) # <--- ä¼ å…¥ labels
    save_to_hdf5(TEST_FILE, test_images, test_masks_task1, test_masks_task2, test_labels) # <--- ä¼ å…¥ labels
    
    print("ğŸ‰ æ•°æ®é›†ç”Ÿæˆå®Œæ¯•ï¼")

if __name__ == "__main__":
    generate_and_save_dataset(TOTAL_SAMPLES, TEST_SPLIT_RATIO)